---
layout: default
title: Locality Sensitive Hashing
comments: true
categories: [algorithm]
published : false
---
## Locality Sensitive Hashing
在大规模的高维数据集中，寻找Top N与某样本最相似的样本集的方法需要遍历所有其他样本，而使用LSH后，新来的样本只需要与某个局部地区的所有样本进行匹配即可找到N紧邻，而不必遍历所有样本.

参考资料：[http://infolab.stanford.edu/~ullman/mmds/ch3a.pdf](http://infolab.stanford.edu/~ullman/mmds/ch3a.pdf)

### 一、问题的出现
假设我们有100万个文档，如果我们要找出与任意一个文档最相似的N个文档，我们需要对每一个文档都进行全遍历的相似度计算.这个工程非常浩大，更别提每个文档中的单词个数可能有非常多个了.

### 二、降维(Minhash)
我们先从简单的骨头啃起，将相似度的计算复杂度降低一些.

假设我们有这么一组文档，$$S_{k}$$ 代表一个文档, `a,b,c,d,e`代表单词的集合:

<img src="/images/algorithm/2-22/1.png"/>

对于其中的每一个样本$$S_{k}$$，我们可以使用一组Hash函数\[$$h_{1},h_{2},...,h_{k}\]对他们进行哈希.

对于每一个文档，我们就可以得到大小为k的向量. 

这些Hash函数很简答, 就是首先将所有文档中的单词进行任意排列组合，得到k个函数. 当对一个文档使用某一个Hash的时候, 遍历当前的排列，遇到的第一个在文档中存在的单词即为返回值.

使用这种方式，我们可以把所有文档的向量缩小为指定长度(k)的向量.

**但是**(没错，就是那个万恶的"但是")

在实践中如果特征的总数太多，比如所有单词的组合数量，这导致我们对特征进行随机的排列组合的代价会比较大，同时还需要对每一个样本进行K次哈希.









