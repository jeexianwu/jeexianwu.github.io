---
layout: default
title: Spark入门
categories: [program]
published: false
---

# Spark入门
-----------------

## 1. 下载和安装
在官网下载后直接在根目录执行`sbt/sbt assembly`下载依赖的jar包(Spark是使用maven进行的依赖管理).

需要注意的是Spark回依赖于特定的scala版本, 比如我的Spark-0.8.0依赖于Scala 2.9.2. 我在执行`sbt/sbt assembly`的时候发现Spark自动下载了相关联的Scala编译器.


安装完成之后可以马上执行测试程序，比如`./run-example org.apache.spark.examples.SparkPi local`, `local`是指本地执行，也可以是一个cluster的master的url. local模式下默认是单线程，也可以指定多个线程 : <pre>local[N]</pre>.

下面是我尝试跑一下例子程序:

	<pre>./run-example org.apache.spark.examples.SparkPi local[2]</pre>

也就是两个线程同事运算Pi的值.

## 2. Spark的一些基本概念

默认情况下Spark原生支持Scala，但我不懂Scala，幸亏Spark提供了Python支持，所以我可以继续做一些试验进行体验.

RDD : 即Resilient Distributed Dataset, 它是Spark中最基本的数据表达, RDD可以从HDFS中的文件创建，也可以从其他RDD互相转换:

	`logData = sc.textFile('README.md').cache()`

上面的代码把`README.md`读并缓存到内存里，方便下次使用, 直接输入`logData`可以看到这是一个RDD对象，也可以通过`help(logData)`查看可以进行哪些操作.

```
	logData
	<pyspark.rdd.RDD object at 0x10f4d9a50>`
```

Action : 即RDD对象可以进行哪些操作，通过`help(logData)`我们已经可以看到里面有哪些action了. 进行Action之后可以返回新的RDD或者直接是结果值.我们从logData里选取包含`sbt`的行, 发现返回的是一个新的RDD对象(RDD的子类PipelinedRDD):

```
 errors = logData.filter(lambda line: "sbt" in line)
 errors
<pyspark.rdd.PipelinedRDD object at 0x10f4c1310>
```

然后通过`collect`操作获得结果列表, Spark会从各个节点获得结果综合起来(现在我们还是单机情况):

<pre>
errors.collect()
</pre>


## 3. Python独立程序
