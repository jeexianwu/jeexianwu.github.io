---
layout: default
title: 线性回归标准方程及其概率解释
comments: true
categories: [ml]
---

## 线性回归标准方程及其概率解释
发布日期：2012-12-7
---

之前学习Coursera机器学习视频的时候，写过一个简单的记录（看这里），感觉实在是理解没到位，现在重新整理一下，希望能完整的解释清楚。

线性回归作为一种监督学习方法，在机器学习领域中属于最基本的优化问题，即根据现有的数据集，找到一个能够最好拟合这组数据的线性函数即可，根据这个线性函数对新来的数据进行预测。

本文将会覆盖最简单的线性回归的解释和标准方程求解最优线性回归参数，至于梯度下降法求解，会有单独的另外一篇博客介绍。

### 什么是回归分析
---
显而易见，线性回归就是一种回归分析，那么什么是回归分析呢？

简单的说，就是找到一条能够最好的代表所有数据的函数，这个函数可以是线性的，当然也可以是非线性的。

而通常情况下数据集并不会是严格的能够使用一条函数代表，所以就会需要我们引入误差的概念，就是说最小化这个误差就行了，通常使用的方法有：**最小二乘法、最大似然估计法**等，后面我们会介绍到，对于线性回归来说这两种方法其实是等价的。

最小二乘法：又称为最小平方法，就是把所有误差的平方相加，获得的值为总误差，最小化这个误差就是优化目标。

最大似然估计：简单的说，就是根据现有数据的分布，估算出一个能够获得该分布情况的分布函数，可以[参考这里](http://zh.wikipedia.org/wiki/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E6%B3%95)详细了解。

### 什么是线性回归(Linear Regression)
---
线性回归就是上面提到的，能够代表现有数据集的函数是线性的，如下图所示：

<img src="/images/ml/12-7/Linear_least_squares.png" width="300"/>

上图中的红色点就表示二维空间的数据集，而蓝色的线就是我们要求解的线性函数。

对于一个线性函数，我们通常的表示方法是：

$$f(x)=wx+b$$

其中w和b都是常量参数，推广到多维空间，该表示方法同样适用：

$$f(x_{1},x_{2}⋯x_{n})=w_{1}x_{1}+w_{2}x_{2}+⋯w_{n}x_{n}+b=\begin{bmatrix}x_{1},x_{2} \cdots x_{n}\end{bmatrix}\cdot \begin{bmatrix} w_{1} \\\\ w_{2} \\\\ \vdots \\\\  w_{n} \end{bmatrix} + b$$

为了统一参数，我们可以为x维度加上一个1，为w维度增加上一个b，这样线性函数就统一成了：

$$ f(x_{1},x_{2} \cdots x_{n}) = \left \[ x_{1},x_{2} \cdots x_{n} ,1 \right \]\cdot \begin{bmatrix} w_{1} \\\\ w_{2} \\\\ \vdots \\\\ w_{n} \\\\ b \end{bmatrix} = {x'}{w'} $$

### 使用标准方程法求解
---
有了线性回归的函数表示，我们的目标自然是获得w的最优值了，根据这个最优值就可以对新来的数据进行预测了。

那么如何获得最优的w呢？我们这里可以使用最小二乘法，误差函数为：

$$L(w)=\sum_{i}(wx_{i}−y_{i})^2$$

最小化线性误差：

$$ \min_{w}L(w) = \min_{w} \sum_{i} (wx_{i} - y_{i})^2 $$

要获得该函数的最小值，只需要对其求w的导数，并令导数为0即可：

$$ \frac{\mathrm{dL(w)} }{\mathrm{d} w} = \begin{bmatrix} \frac{\partial L(w)}{\partial w_{1}} \\\\ \frac{\partial L(w)}{\partial w_{2}} \\\\ \vdots \\\\ \frac{\partial L(w)}{\partial w_{n}} \end{bmatrix} = 2\sum_{i}x_{i}x_{i}^{T}w - 2\sum_{i}x_{i} y_{i} = 0 $$

即求L(w)关于w的各个维度的偏导数，然后求和即可。

为了表示方便，我们假设有n个样本，这些样本的属性集合，以及他们的结果值的集合分别为：

$$ X = \begin{bmatrix}x_{1} \\\\ x_{2} \\\\ \vdots \\\\ x_{n}\end{bmatrix} , Y = \begin{bmatrix}y_{1} \\\\ y_{2} \\\\ \vdots \\\\ y_{n}\end{bmatrix} $$

即上面的导数可以重新写为：

$$ \frac{\mathrm{dL(w)} }{\mathrm{d} w} = 2\sum_{i}x_{i}x_{i}^{T}w - 2\sum_{i}x_{i} y_{i} = 2X^TXw - 2X^TY=0 $$

由于矩阵的逆与原矩阵相乘为1，所以上式求解可得：

$$w^\* = (X^TX)^{-1}X^TY$$

$w^\*$即为我们要求的最优值了。

#### 第一个问题
---
有了上面的求解w\*公式，我们只需要遍历所有训练数据集，将数据读出来后进行矩阵运算即可。也正因为如此，第一个问题来了，假如我们的数据维度特别高，那么这种矩阵运算是十分耗费时间的，即便使用一些如分治法的策略，也只能很有限的降低计算复杂度。

所以标准方程的方式，只适合于数据量和数据维度不是非常大的情况，否则更建议使用梯度下降法进行计算，关于梯度下降法，后面我会再写一篇文章专门介绍。

#### 第二个问题
---
假如 $ X^TX $ 不可逆怎么办，这个问题比较关键，为了解决这个问题，研究者为求解w\*的函数引入了对角矩阵作为参数来保证 $ X^TX $可逆，即原求解公式变为：

$$w^* = (X^TX + \lambda I)^{-1}X^TY$$

其中$\lambda I$为形如下列格式的单位对角矩阵：

$$\lambda \begin{bmatrix}1 &0 &0 &0 \\\\ 0 &1 &0 &0 \\\\ 0 &0 &1 &0 \\\\0 &0 &0 &1 \end{bmatrix}$$

这种方法求出的w\*一定是最优的。

那么，引入这么一个参数，必然要有其具体的函数意义，比如高斯分不中引入噪声、SVM中引入松弛变量，那么这里的 $ \lambda I $表示什么含义呢。

### 概率解释
---
标准方程获得的结果可以从很多个角度来解释其意义，我们可以选择从概率的角度来看（但这个式子并非是从概率论推出来的，具体如何来的笔者也暂不知道......只是前辈告诉我就是突然出现这么一个式子比较合理，从各个角度都能解释的通）。

我们这里从三个不同的角度来看待线性回归的参数估计问题，他们分别是：最大似然估计、贝叶斯估计和最大后验估计。

#### 一）最大似然估计
---
最大似然估计比较简单，它假定要估计的模型参数w虽然是未知的，但应该当确定值，然后找到符合对数似然最大分布的参数值。

首先，我们假设线性模型为：

$$y = wx+\varepsilon$$

其中$ \delta $是线性样本上的噪声数据，我们假设噪声符合高斯分布，即

$$\varepsilon\sim N(0,\delta ^2)$$

在确定w和x的情况下，我们可以看到y与$\varepsilon$分布相同，也就是说对于每一个数据点：

$$p(y_{i}|x_{i};w) = p(\varepsilon ) = \frac{1}{\sqrt{2\pi }\delta }exp(- \frac{\varepsilon^2}{2\delta^2}) $$

由于$\varepsilon=y-wx$，所以：

$$p(y_{i}|x_{i};w) = \frac{1}{\sqrt{2\pi }\delta }exp(- \frac{(y_{i}-wx_{i})^2}{2\delta^2})$$

对于所有的样本X，以及他们所有的预测结果Y有：

$$P(Y|X;w) = \prod p(y_{i}|x_{i};w)$$

最大化这个概率，求出w就是我们的目标，为了方便计算，我们把上式的连乘转换为**负对数最大似然**求极小值问题，把p值代入即可得：

$$L(w)= - \sum_{i} log(p(y_{i}|x_{i};w)) = \frac {1}{2\delta ^2}\sum_{i}(y_{i}-wx_{i})^2 + C$$

由于$\delta$、$C$都是常数，所以最小化这个$L(w)$与最小二乘法的结果一样。

现在我们知道了为什么标准方程中使用差平方的和是合理的，而不是使用其他的误差计算方式。

### 二）贝叶斯最大后验估计
---
贝叶斯估计的特点是，假设已知样本处于分布D上（如高斯分布），根据已有样本计算在分布D上概率最大的参数w。

给定X、Y推导w的贝叶斯后验概率：$$p(w|X,Y) = \frac {p(w,Y|X)}{p(Y|X)}$$

由于贝叶斯的特点就是假设样本分布于D上，所以我们这里假设该分布为高斯分布，那么参数w在该分布上的概率为$w|X\sim N(0,\gamma^2)$，即：

$$p(w|X) = \frac {1}{\gamma\sqrt{2\pi}}exp(-\frac {w^2}{2\gamma^2})$$

则，贝叶斯后验概率可以转换为：

$$p(w|Y,X) = \frac {p(w,Y|X)}{p(Y|X)} = \frac {p(Y|w,X)p(w|X)}{\int p(Y|w,X)p(w|X)dw}$$

根据贝叶斯定理和条件概率的基本性质([维基百科](http://zh.wikipedia.org/wiki/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9A%E7%90%86))，我们可以把上市做一些转换，因为我们已经预先假设了$p(w|X)$，所以要转换结果有它。

最大化$p(w|Y,X)$可以表示为：

$$max_{w} p(w|Y,X) =max_{w} \frac {p(w,Y|X)}{p(Y|X)} = \frac {p(Y|w,X)p(w|X)}{p(Y|X)}$$

由于$p(Y|X)$为常数，所以上式等价于$max_{w} p(Y|w,X)p(w|X)$ 最小化其负对数后验概率：

$$min_{w}-log(p(Y|w,X)p(w|X)) = min_{w} -log(p(Y|w,X)) - log(p(w|X))$$

$$= -\sum log(p(y_{i}|w,x_{i}))-log(p(w|X))$$

$$= \frac {1}{\delta ^2}\sum(y_{i}-wx_{i})^2 + \frac {1}{\gamma^2}w^2$$

如果我们令$\lambda=\frac {\delta^2}{\gamma^2}$，则上式等价于:

$$min_{w} \sum(y_{i}-wx_{i})^2 + \lambda w^2$$

令其全导数为零，则最优解为：

$$w^\*=(X^TX+\lambda I)^{-1}2X^TY$$

我们发现贝叶斯最大后验概率与标准方程形式相同，从概率的角度是有意义的。

$\lambda I$的含义，其中I是单位对角矩阵，


### 后记
---

一个简单的线性回归，里面涉及的知识还是很深的，不时回头回顾总能发现新的东西，学无止境啊。


